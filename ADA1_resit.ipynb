{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba36903-ee71-4a62-9bb8-8e0270fdae64",
   "metadata": {},
   "source": [
    "# Advanced Text Analytics Lab 1 - Resit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de31fe1-70a5-4357-b9d9-be40038c556c",
   "metadata": {},
   "source": [
    "This notebook is the first of two lab notebooks that you will submit as part of your assessment for the Advanced Data Analytics unit. \n",
    "\n",
    "This notebook is contains three sections:\n",
    "1. **Word embeddings:** This will introduce you to loading and training word embeddings using the Gensim library.\n",
    "2. **Introducing neural text classifiers:** Here we show you how to construct a neural network text classifier for sentiment analysis using Pytorch. \n",
    "3. **Improving neural text classifiers:** This section gives you a chance to improve the classifier from the previous section by applying what we have learned in the lectures.\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "These sections will contain tutorial-like instructions, as you have seen in previous text analytics labs. On completing these sections, the intended learning outcomes are that you will be able to...\n",
    "1. Load pretrained word embeddings models.\n",
    "1. Learn word embeddings from an unlabelled dataset.\n",
    "1. Recognise the steps required to train and test a neural text classifier with Pytorch\n",
    "1. Adapt the architecture of a neural text classifier.\n",
    "\n",
    "## Your Tasks\n",
    "\n",
    "Inside each of these sections there are several **'To-do's**, which you must complete for your summative assessment. Your marks will be based on your answers to these to-dos. Please make sure to:\n",
    "1. Include the output of your code in the saved notebook. Plots and printed output should be visible without re-running the code. \n",
    "1. Include all code needed to generate your answers.\n",
    "1. Provide sufficient comments to understand how your method works.\n",
    "1. Write text in a cell in markdown format where a written answer is required. You can convert a cell to markdown format by pressing Escape-M. \n",
    "\n",
    "There are also some unmarked 'to-do's that are part of the tutorial to help you learn how to implement and use the methods studied here.\n",
    "\n",
    "## Marking Criteria\n",
    "\n",
    "1. The coursework (both notebooks) is worth 30% of the unit in total. \n",
    "1. There is a total of 100 marks available for both lab notebooks. \n",
    "1. This notebook is worth 66 of those marks.\n",
    "1. The number of marks for each to-do out of 100 is shown alongside each to-do.\n",
    "1. For to-dos that require you to write code, a good solution would meet the following criteria (in order of importance):\n",
    "   1. Solves the task or answers the question asked in the to-do. This means, if the code cells in the notebook are executed in order, we will get the output shown in your notebook.\n",
    "   1. The code is easy to follow and does not contain unnecessary steps.\n",
    "   1. The comments show that you understand how your solution works.\n",
    "   1. A very good answer will also provide code that is computationally efficient but easy to read.\n",
    "1. You can use any suitable publicly available libraries. Unless the task explicitly asks you to implement something from scratch, there is no penalty for using libraries to implement some steps.\n",
    "\n",
    "## Support\n",
    "\n",
    "The lecturer will help you with questions about the lectures, the code provided for you in this notebook, and general questions about the topics we cover. For the marked 'to-dos', they can only answer clarifying questions about what you have to do. \n",
    "\n",
    "Office hours: You can book office hours with Edwin on Tuesdays 3pm-5pm by sending him an email (edwin.simpson@bristol.ac.uk). If those times are not possible for you, please contact him by email to request an alternative. \n",
    "\n",
    "## Deadline\n",
    "\n",
    "The notebook must be submitted along with the second notebook on Blackboard before **Monday 8th August at 13.00**. \n",
    "\n",
    "## Submission\n",
    "\n",
    "You will need to zip up this notebook and the next notebook into a single .zip file, which you will submit to Blackboard through the 'assessment, submission and feedback' link on the left sidebar. \n",
    "\n",
    "Please name your files like this:\n",
    "   * Name this notebook ADA1_\\<student_number\\>.ipynb\n",
    "   * Name the zip file \\<student_number\\>.zip\n",
    "   * Please don't use your name anywhere as we want to mark anonymously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c88b74-685a-4324-a340-b5c4fa5301d1",
   "metadata": {},
   "source": [
    "# 0. Packages\n",
    "\n",
    "There are some additional packages you need for this lab. There are two options: set up a new environment using the crossplatform_environment.yml file provided on Blackboard, or install the extra packages into your existing data_analytics Conda environment using, e.g., `conda install pytorch`. The packages are:\n",
    "  * pytorch=1.9.0\n",
    "  * scipy=1.8.0\n",
    "  * numpy=1.22.2\n",
    "  * tqdm=4.62.3\n",
    "  * transformers=2.1.1\n",
    "  * matplotlib-base=3.5.1\n",
    "  * matplotlib-inline=0.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549d0b9-84d1-4d35-80b1-6341a3ea7ce6",
   "metadata": {},
   "source": [
    "# 1. Word Embeddings (max. 26 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a8852-b72a-48dd-a266-25a4bb774f6a",
   "metadata": {},
   "source": [
    "In this section we will use both sparse vectors and dense word2vec embeddings to obtain\n",
    "vector representations of words and documents. \n",
    "\n",
    "First, we will load the [`tweets hate speech` dataset](https://huggingface.co/datasets/tweets_hate_speech_detection). This dataset contains tweets with binary labels, with 1 indicating hate speech and 0 indicating no hate speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519c4b9-1cac-4e8a-9e7e-a8a40b389bdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599bf09-ed1a-4e40-a551-2e3a889a3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweets_hate_speech_detection\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_dataset['tweet'], train_dataset['label'], test_size=0.2)\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(train_texts, train_labels, test_size=0.2)\n",
    "\n",
    "# HINT: A count vectorizer object may be useful in later steps\n",
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = vectorizer.vocabulary_\n",
    "vocab_size = len(vocab)\n",
    "print(f'The vocabulary has {vocab_size} words')\n",
    "\n",
    "# invert the vocabulary dictionary so we can look up word types given an index\n",
    "keys = vocab.values()\n",
    "values = vocab.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "print(f'Index of \"happy\" is {vocab[\"happy\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a63c3c-d24c-4810-9c34-4532b845e310",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Term-Document Matrix\n",
    "\n",
    "First we are going to obtain sparse word vectors from a term-document matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd30abb-0951-46ee-81a6-b9217fba6b90",
   "metadata": {},
   "source": [
    "**TO-DO 1.1a**: Create a term-document matrix for the training set. **Rows must correspond to terms, and columns to documents.** **(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a52da7-89a5-41d6-bfab-83b3231d6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d8c39-99e0-4113-901f-b4cd72b2e369",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 1.1b:** Write a function that takes a word as an argument and returns a list of document indexes for documents containing that term. Use the function to print a list of indexes of tweets containing the word 'love'. **(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2495952-e973-4b68-b219-590791740daf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb324d8-a92a-4fd7-bd7f-f3b0f0958e6b",
   "metadata": {},
   "source": [
    "**TODO 1.1c:** Use the term-document matrix to identify the most similar document to document 0. Remember that the columns of the term-document matrix are document vectors. Print both document 0 and its most similar document. **(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cca67-685e-4760-92be-eea50ed87769",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist  # this may be a useful function \n",
    "\n",
    "# WRITE YOUR OWN CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9654fde-cbe7-4020-99f7-49d97b4eacfa",
   "metadata": {},
   "source": [
    "## 1.2 Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e772ce7-853e-478d-9d0a-3f591100e88d",
   "metadata": {},
   "source": [
    "Now, we will use Gensim to train a word2vec model. The code below tokenizes the training texts, then runs word2vec (the skipgram model) to learn a set of embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d13e1-683f-41f6-bf24-7c036aef59fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "tokenized_texts = [list(tokenize(text)) for text in train_texts]\n",
    "emb_model = word2vec.Word2Vec(tokenized_texts, sg=1, min_count=1, window=3, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989624ac-dc98-461f-a5ad-c28cf27ca085",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the word vector for 'love'\n",
    "happy_embedding = emb_model.wv['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd66ea-cbbe-4c1a-a467-5a784facf33b",
   "metadata": {},
   "source": [
    "TODO 1.2a: Find the five most similar words to 'love' according to your word2vec model. You can use the Gensim function `similar_by_word` to do this. (this task is unmarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37a093-db74-4e10-9224-0740381b30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28de29-ee93-47e9-ba62-de7771d7203d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Above, we trained our own model using the skipgram method. We can also download a pretrained model that has previously been trained on a large corpus. There is a list of models available [here](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models). Let's try out GLoVe embeddings. GLoVe is an alternative to the skipgram model. This model was trained on a corpus of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693126a6-519a-44c1-b800-773fc95d38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "glove_wv = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "# show the vector for Hamlet:\n",
    "print(glove_wv['love'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbeb21-549d-4315-bce1-e785cda13fd2",
   "metadata": {},
   "source": [
    "TODO 1.2b: Find the most similar five words to 'happy' according to the GloVe Twitter model. (this task is unmarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893470a-08b7-4f44-b5f0-8faddb91ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4dc0e-7c98-49df-a316-3d67b5a95ca1",
   "metadata": {},
   "source": [
    "# 1.3 Tweet Embeddings\n",
    "\n",
    "For many tasks it is useful to obtain a document embedding that characterises the meaning of a document. With short documents, such as tweets, a reasonable way to obtain a document embedding is to average the word embeddings of the tokens in the tweet.\n",
    "\n",
    "**TO-DO 1.3a:** Compute average word embeddings for each tweet in the `tokenized_texts` list to compute a representation for each tweet. You can use any of the word embeddings we have generated so far. **(4 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c8000-a181-4137-b6ca-f7a5a8796928",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wv[tokenized_texts[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddecdad-b333-4a71-abf7-16a37fda14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7b648-b65c-4c2f-bc34-c3ce1cf362cb",
   "metadata": {},
   "source": [
    "**TO-DO 1.3b:** What are the limitations of using average word embeddings to represent documents? Explain in a couple of sentences. You can write your answer inside this cell. **(4 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8938c15-1aa4-4a2f-b1e6-33c5155fb085",
   "metadata": {},
   "source": [
    "**TO-DO 1.3c:** Use the embeddings to organise the tweets in `tokenized_texts` into topic categories. There are no gold labels for the categories, so you will need to choose a method that does not require them. Show or visualise the words that are most strongly associated with each category. Based on these words, can you identify a topic name for any of the categories? Describe what you find in a couple of lines below. **(10 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8b106-afad-4159-bb59-2658053db35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d24e4-d90d-4f50-a167-5f4876076355",
   "metadata": {},
   "source": [
    "# 2. Introducing Neural Text Classifiers (max. 18 marks)\n",
    "\n",
    "This section shows you how to implement a neural network classifier using Pytorch and leads you through the steps required to process text sequences.\n",
    "\n",
    "There are several big advantages to building a text classifier using a neural network:\n",
    "   * It can model nonlinear functions, so can handle much more complex relationships between features and class labels.\n",
    "   * It performs representation learning: the hidden layers learn how to extract features from low-level data.\n",
    "   * It can process sequences of tokens -- we don't have to think in terms of a single feature vector representing a document as we did for logistic regression.\n",
    "  \n",
    "The downsides are:\n",
    "   * Much more expensive to train and test.\n",
    "   * It can overfit very badly to small datasets.\n",
    "   * The features learned by the hidden layers can be hard to interpret.\n",
    "   \n",
    "Let's start by building a neural network text classifier that takes a sequence of tokens as input, and predicts a class label. For simplicity, it will use a single fully connected feedforward layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4f88e-bd86-4917-8e9d-26ace635873e",
   "metadata": {},
   "source": [
    "The first step -- as always -- is to get our data into the right format. We start from a set of tokenised documents (in this case, tweets), where each document is represented as a sequence of text tokens. The neural network cannot process the tokens as strings, so we need to convert them to numerical data.\n",
    "\n",
    "We are going to construct the neural network in this form:\n",
    "\n",
    "<img src=\"neural_text_classifier_smaller.png\" alt=\"Neural text classifier diagram\" width=\"600px\"/>\n",
    "\n",
    "The input value for each token is used to look up the corresponding embedding in the embedding layer. For PyTorch, it's not necessary to create the one-hot vectors as the embedding lookup is handled inside the library. Instead, we just need to give the indexes of the words in the vocabulary.\n",
    "\n",
    "So, let's now map the tokens to their IDs -- their indexes in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a71101-e504-42a8-9862-74796af52cd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize training set and convert to input IDs.\n",
    "def encode_text(tweet):\n",
    "    tokens = tokenize(tweet)  # Tokenize one document\n",
    "    \n",
    "    input_ids = []\n",
    "    for token in tokens:\n",
    "        if str.lower(token) in vocab:  # Skip words from the dev/test set that are not in the vocabulary.\n",
    "            input_ids.append(vocab[str.lower(token)]+1) # +1 is needed because we reserve 0 as a special character\n",
    "            \n",
    "    return input_ids\n",
    "\n",
    "train_ids = [encode_text(tweet) for tweet in train_texts]\n",
    "len(train_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce8d8c-ce83-49b7-9b88-b84233be7dbf",
   "metadata": {},
   "source": [
    "Our neural network's input layer has a fixed size, so we need to make all of our documents have the same number of tokens. Let's plot a histogram to understand the length distribution of the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832da949-3fec-4d26-936e-23c943546b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rv_l = [len(doc) for doc in train_ids]\n",
    "print('Mean of the document length: {}'.format(np.mean(rv_l)))\n",
    "print('Median of the document length: {}'.format(np.median(rv_l)))\n",
    "print('Maximum document length: {}'.format(np.max(rv_l)))\n",
    "\n",
    "plt.hist(rv_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97d92c-7002-4b63-9011-8ed21ea52e95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now neeed to choose a fixed sequence length, then *pad* the documents that are shorter than this maximum by adding a special token to the start of the sequence. Any documents that exceed the length will be truncated.\n",
    "\n",
    "**TO-DO 2a:** Complete the padding code below to insert 0s at the start of any sequences that are too short, and to truncate any sequences that are too long. **(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8488dfa-61b8-438e-92ee-7f9b84b27a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_length = 20  # truncate all docs longer than this. Pad all docs shorter than this.\n",
    "\n",
    "def pad_text(input_ids):\n",
    "    ###WRITE YOUR OWN CODE HERE\n",
    "\n",
    "\n",
    "    ##########\n",
    "    return np.array(input_ids)\n",
    "\n",
    "# The will call pad_text for every document in the dataset\n",
    "train_ids = [pad_text(input_ids) for input_ids in train_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb33a1-ca74-4a95-bbd5-2b9e50167733",
   "metadata": {},
   "source": [
    "We now have our data in almost the right format! To train a model using PyTorch, we are going to wrap our dataset in a [DataLoader object](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). This allows the training process to select random subsets of the dataset -- mini-batches -- which it will use for learning with a mini-batch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3c226-402b-4eb5-8f9a-fe216b4c5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# convert from the Huggingface format to a TensorDataset so we can use the mini-batch sampling functionality\n",
    "def convert_to_data_loader(ids, labels, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(np.array(ids))\n",
    "    label_tensor = torch.from_numpy(np.array(labels)).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "num_classes = len(np.unique(train_labels))   # number of possible labels in the sentiment analysis task\n",
    "\n",
    "train_loader = convert_to_data_loader(train_ids, train_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5825c-4a0e-4fc8-9de6-504bc7227377",
   "metadata": {},
   "source": [
    "Let's process the development and test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa6ddc-b936-431c-865e-dc55eaf3086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ids = [pad_text(encode_text(tweet)) for tweet in dev_texts] \n",
    "dev_loader = convert_to_data_loader(dev_ids, dev_labels, num_classes)\n",
    "\n",
    "test_ids = [pad_text(encode_text(tweet)) for tweet in test_texts] \n",
    "test_loader = convert_to_data_loader(test_ids, test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b857f-5d57-44fd-a1b4-e1b31a009c72",
   "metadata": {},
   "source": [
    "As shown in the diagram above, we will build a NN with three different layers for sentiment classification.\n",
    "\n",
    "### Embedding layer\n",
    "In the embedding layer, the network will create its own embeddings for the index with a given embedding dimension.\n",
    "The module `nn.Embedding()` creates a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "This module is often used to store word embeddings and retrieve them using indices.\n",
    "The module's input is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "[Documentation for Embedding Class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "### Fully-connected layer\n",
    "Fully-connected layers in a neural network are those layers where all the inputs from the previous layer are connected to every unit of the fully-connected layer. Here we will use fully-connected layers for the hidden layer and output layer. In Pytorch this kind of layer is implemented by the 'Linear' class:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "## Activation functions\n",
    "In Pytorch, the activation function is not included in the Linear class (or other kinds of neural network layer).\n",
    "In Pytorch, we construct a neural network by connecting up the output of each component to the input of the next, thereby creating a computation graph.\n",
    "To complete the hidden layer, we connect the ouput of the linear layer to a ReLU activation function, thereby creating a nonlinear function.\n",
    "\n",
    "The cell below defines a class for our neural text classifier. The constructor creates each of the layers and the activations. The dimensions of each layer need to be correct so that the output of one layer can be passed as input to the next.\n",
    "\n",
    "**TO-DO 2b** Complete the constructor below for a NN with three layers by adding the missing dimensions. **(2 marks)**\n",
    "\n",
    "Below is the forward method. This is called to map the neural network's inputs to its outputs. In PyTorch, we pass data through each layer of the model, connecting them together.\n",
    "\n",
    "**TO-DO 2c** Complete the forward method by adding the missing lines. **(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a13cf-4b4d-4ac2-868c-8ad5c1b72b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FFTextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, sequence_length, embedding_size, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        ### COMPLETE THE CODE HERE: WRITE IN THE MISSING ARGUMENTS SPECIFYING THE DIMENSIONS OF EACH LAYER\n",
    "        self.embedding_layer = nn.Embedding(...) # embedding layer\n",
    "        self.hidden_layer = nn.Linear(...) # Hidden layer\n",
    "        self.activation = nn.ReLU(...) # Hidden layer\n",
    "        self.output_layer = nn.Linear(...) # Full connection layer\n",
    "\n",
    "        ##########\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        # flatten the sequence of embedding vectors for each document into a single vector.\n",
    "        embedded_words = embedded_words.reshape(embedded_words.shape[0], self.sequence_length*self.embedding_size)  # batch_size, seq_length*embedding_size\n",
    "\n",
    "        ### ADD THE MISSING LINES HERE\n",
    "\n",
    "        \n",
    "        ########\n",
    "\n",
    "        output = self.output_layer(h)                      # (batch_size, num_classes)\n",
    "\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f22db-b953-4f98-920f-3f9a2b1e3707",
   "metadata": {},
   "source": [
    "Now the class is complete...\n",
    "\n",
    "TO-DO 2d: Create a NN with the FFTextClassifier class we wrote. (unmarked)\n",
    "\n",
    "**Hint:** `model = FFTextClassifier(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d795a-adbc-4c48-aadf-f911520ac42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vectorizer.vocabulary_) + 1\n",
    "embedding_size = 10  # number of dimensions for embeddings\n",
    "hidden_size = 8 # number of hidden units\n",
    "\n",
    "###WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6c48d-f2b4-45e6-838e-d08c895f7e86",
   "metadata": {},
   "source": [
    "After desigining our network, we need to create a training function to calculate the loss for each input and perform backpropagation to optimise the network.\n",
    "During training, the weights of all the layers will be updated.\n",
    "\n",
    "Below, we build a training function to train the NN over a fixed number of epochs (an epoch is one iteration over the whole training dataset).\n",
    "The function also prints the performance of both training and development/validation set after each epoch.\n",
    "\n",
    "Here we use cross-entropy loss, which is the standard loss function for classification that we also used for logistic regression. The module `nn.CrossEntropyLoss()` operates directly on the output of our output layer, so we don't have to implement the softmax layer within the forward() method.\n",
    "\n",
    "Cross Entropy Loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "The optimizer object implements a particular algorithm for updating the weights. Here, we will use the Adam optimizer, which is a variant of stochastic gradient descent method that tends to find a better solution in a small number of iterations than standard SGD.\n",
    "\n",
    "Optimization: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "**TO-DO 2e** Modify the code below to save the average dev/validation loss after each training epoch. **(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af0a20-79a5-49ca-882a-638a68b288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def train_nn(num_epochs, model, train_dataloader, dev_dataloader):\n",
    "    \n",
    "    learning_rate = 0.0005  # learning rate for the gradient descent optimizer, related to the step size\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()  # create loss function object\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # create the optimizer\n",
    "       \n",
    "    dev_losses = []\n",
    "        \n",
    "    for e in range(num_epochs):\n",
    "        # Track performance on the training set as we are learning...\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()  # Put the model in training mode.\n",
    "\n",
    "        for i, (batch_input_ids, batch_labels) in enumerate(train_dataloader):\n",
    "            # Iterate over each batch of data\n",
    "            # print(f'batch no. = {i}')\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the optimizer\n",
    "\n",
    "            # Use the model to perform forward inference on the input data.\n",
    "            # This will run the forward() function.\n",
    "            output = model(batch_input_ids)\n",
    "\n",
    "            # Compute the loss for the current batch of data\n",
    "            batch_loss = loss_fn(output, batch_labels)\n",
    "\n",
    "            # Perform back propagation to compute the gradients with respect to each weight\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update the weights using the compute gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss from this sample to keep track of progress.\n",
    "            train_losses.append(batch_loss.item())\n",
    "\n",
    "            # Count correct labels so we can compute accuracy on the training set\n",
    "            predicted_labels = output.argmax(1)\n",
    "            total_correct += (predicted_labels == batch_labels).sum().item()\n",
    "            total_trained += batch_labels.size(0)\n",
    "\n",
    "        train_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Training Loss: {:.4f}\".format(np.mean(train_losses)),\n",
    "              \"Training Accuracy: {:.4f}%\".format(train_accuracy))\n",
    "\n",
    "        model.eval()  # Switch model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "\n",
    "        dev_losses_epoch = []\n",
    "        \n",
    "        for dev_input_ids, dev_labels in dev_dataloader:\n",
    "            dev_output = model(dev_input_ids)\n",
    "            dev_loss = loss_fn(dev_output, dev_labels)\n",
    "\n",
    "            ### Add your own code here ###\n",
    "\n",
    "            ####\n",
    "\n",
    "            # Count the number of correct predictions\n",
    "            predicted_labels = dev_output.argmax(1)\n",
    "            total_correct += (predicted_labels == dev_labels).sum().item()\n",
    "            total_trained += dev_labels.size(0)\n",
    "            \n",
    "        ### Add your own code here \n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Validation Loss: {:.4f}\".format(dev_losses[-1]) )\n",
    "\n",
    "    return model, dev_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e473f92-78d4-4a5d-94cc-749165e3896e",
   "metadata": {},
   "source": [
    "**TO-DO 2f:** Suppose you are tracking the validation loss during training, and see that it hasn't decreased for two epochs, even though the training loss has decreased. What do you think is happening, and how could you use this information to improve the training process? **(3 marks)**\n",
    "\n",
    "EXPLAIN YOUR ANSWER HERE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639b1e0-c968-4086-ada2-21df7100acb9",
   "metadata": {},
   "source": [
    "TO-DO 2g: Finally, train the network for 10 epochs! (this to-do will not be marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7c289-fae8-4045-b18c-8db7e678214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###WRITE YOUR OWN CODE HERE\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655f07b-c687-4d9b-a0b3-44f0d3ccbead",
   "metadata": {},
   "source": [
    "**TO-DO 2h:** Plot the validation losses. **(2 mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7339f-a51b-48cf-8cc9-067f18e1aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your own code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94feb35a-2783-491f-b625-58ae2294623f",
   "metadata": {},
   "source": [
    "The code below obtains predictions from our neural network. \n",
    "\n",
    "**TO-DO 2i:** Evaluate the model on test set using the function below. Complete the code to compute a suitable performance metric for sentiment classification. **(1 mark)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf4368a8-57fc-4cd3-b74e-aa385105c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_loader):\n",
    "\n",
    "    trained_model.eval()\n",
    "\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    gold_labs = []  # gold labels to return\n",
    "    pred_labs = []  # predicted labels to return\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "\n",
    "        gold_labs.extend(labels.tolist())\n",
    "        pred_labs.extend(predicted_labels.tolist())\n",
    "    \n",
    "    return gold_labs, pred_labs\n",
    "\n",
    "gold_labs, pred_labs = predict_nn(trained_model, test_loader)\n",
    "\n",
    "### WRITE YOUR OWN CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5620658-9e5a-4248-aa48-3e3868011aaf",
   "metadata": {},
   "source": [
    "Now, we can use pretrained word embeddings instead of learning them from scratch during training.\n",
    "Here, we will use the pretrained GloVe embeddings that we loaded before. The embedding matrix is used to initialise the embedding layer. The code below converts the GloVe embeddings into an embedding matrix suitable for PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872638e9-f86c-48e4-8745-9c667e250a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.zeros((vocab_size, glove_wv.vector_size))\n",
    "for word in vocab:\n",
    "    word_idx = vocab[word]\n",
    "    if word in glove_wv:\n",
    "        embedding_matrix[word_idx, :] = torch.from_numpy(glove_wv[word])\n",
    "        \n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e7656-343f-449c-9329-411ad7fb22d5",
   "metadata": {},
   "source": [
    "The class below extends the FFTextClassifier class. This means that it inherits all of its functionality, but we overwrite the constructor (the `__init__` method). This way, we don't need to define the forward function again, as it will be the same as before.\n",
    "\n",
    "The embedding layer is now different as it loads pretrained embeddings from our matrix. The argument `freeze` determines whether the embeddings remain fixed to their pretrained values (if `freeze=True`) or are updated through backpropagation to fit them to the dataset.\n",
    "\n",
    "**TO-DO 2j:** Complete the arguments below to set the dimensions of the neural network layers.  Repeat the experiment above using the FFTextClassifierWithEmbeddings with the GLoVe embeddings. Compare the performance of the two neural text classifiers, and explain in a couple of sentences why you think they perform differently. **(3 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b93f97-57b9-4d3b-a36f-547d472e1f37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "\n",
    "class FFTextClassifierWithEmbeddings(FFTextClassifier):\n",
    "\n",
    "    def __init__(self, vocab_size, sequence_length, embedding_size, hidden_size, num_classes):\n",
    "        super(FFTextClassifierWithEmbeddings, self).__init__(vocab_size, sequence_length, embedding_size, hidden_size, num_classes)\n",
    "\n",
    "        self.embedding_size = embedding_matrix.shape[1] \n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=True) # embedding layer\n",
    "\n",
    "        ### COMPLETE THE ARGUMENTS TO SPECIFY THE DIMENSIONS OF THE LAYERS\n",
    "        self.hidden_layer = nn.Linear(...) # Hidden layer\n",
    "        self.activation = nn.ReLU(...) # Hidden layer\n",
    "        self.output_layer = nn.Linear(...) # Fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4afd4-a92d-468d-a6ea-e693c10de36d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c20d5-12c8-4820-89e2-e53c11b77b42",
   "metadata": {},
   "source": [
    "# 3. Improving the Neural Text Classifier (max. 22 marks)\n",
    "\n",
    "This section allows you some more free reign to experiment with the neural text classifier. Below, we list several to-dos that you can solve in your own way. Please make sure to label your notebook cells clearly so that it is obvious which to-do each cell corresponds to.\n",
    "\n",
    "**TO-DO 3a:** Consider the neural text classifier we have just implemented. It has a number of limitations that we could improve. Describe three limitations and how you could improve them. For each improvement you propose, provide a brief explanation (up to 1 paragraph) of how it works. **(9 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "---\n",
    "\n",
    "**TO-DO 3b:** Implement your improvements and compute the performance of your method. Make sure to comment your code to show where each new step is implemented. Use the validation set for any tuning you decide to do. Present your results clearly. **(13 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e0d0d-ac93-4f7b-8169-c81f6b151eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda1451-426a-445d-8a8e-2ed5585af0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
